{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iJRN9n1TTjzE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, neurons_per_layers, inputs, num_epochs):\n",
        "    self.num_layers = len(neurons_per_layers)\n",
        "    self.inputs = inputs # shape: (batch_size, num_features)\n",
        "    self.current_input = inputs\n",
        "    self.layers = []\n",
        "    self.outputs = None\n",
        "    self.dZ_prev = None\n",
        "    self.learning_rate = 0.01\n",
        "    self.num_epochs = num_epochs\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      num_features = inputs.shape[1] if i == 0 else neurons_per_layers[i-1][0]\n",
        "      newLayer = Layer(neurons_per_layers[i][0], num_features, neurons_per_layers[i][1])\n",
        "      self.layers.append(newLayer)\n",
        "\n",
        "    self.loss = None\n",
        "    pass\n",
        "\n",
        "  def forward_pass(self):\n",
        "    self.current_input = self.inputs\n",
        "    for i in range(self.num_layers):\n",
        "      is_first_layer = i == 0\n",
        "      is_last_layer = i == self.num_layers - 1\n",
        "      self.current_input = self.layers[i].calculate_output(self.current_input, is_first_layer, is_last_layer)\n",
        "\n",
        "    self.outputs = self.current_input\n",
        "    return self.outputs\n",
        "\n",
        "\n",
        "  def back_propagate(self, truth):\n",
        "    self.dZ_prev = self.outputs\n",
        "    for i in range(self.num_layers - 1, -1, -1):\n",
        "      is_first_layer = i == 0\n",
        "      is_last_layer = i == self.num_layers - 1\n",
        "\n",
        "      self.dZ_prev = self.layers[i].loss_function(self.dZ_prev, truth, self.learning_rate, is_last_layer)\n",
        "\n",
        "  def train(self, truth):\n",
        "    for epoch in range(self.num_epochs):\n",
        "      self.forward_pass()\n",
        "      self.calculate_loss(truth)\n",
        "\n",
        "      if self.loss < 0.0001:\n",
        "        break\n",
        "\n",
        "      self.back_propagate(truth)\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {self.loss}\")\n",
        "\n",
        "  def calculate_loss(self, truth):\n",
        "    self.loss = np.mean((self.outputs - truth) ** 2)\n",
        "    return self.loss\n",
        "\n",
        "\n",
        "  def summary(self):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "        print(f\"Layer {i+1}: {layer.num_neurons} neurons, activation: {layer.activation}\")\n",
        "\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, num_neurons, num_features, activation_function):\n",
        "    self.num_neurons = num_neurons\n",
        "    self.weights = np.random.rand(num_features, num_neurons)\n",
        "    self.biases = np.zeros((1, num_neurons))\n",
        "    if isinstance(activation_function, str):\n",
        "      self.activation = activation_function\n",
        "    else:\n",
        "      self.activation = \"sigmoid\"\n",
        "\n",
        "    self.loss = None\n",
        "    self.inputs = None\n",
        "    self.unactivated_outputs = None\n",
        "    self.outputs = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "    pass\n",
        "\n",
        "  def calculate_output(self, input, is_first_layer = False, is_last_layer = False):\n",
        "    self.inputs = input\n",
        "    self.unactivated_outputs = np.dot(input, self.weights) + self.biases\n",
        "    self.outputs = self.activation_function(self.unactivated_outputs, self.activation, is_first_layer, is_last_layer)\n",
        "    return self.outputs\n",
        "\n",
        "  def loss_function(self, dZ_prev, truth, lr, is_last_layer = False):\n",
        "    dA = None\n",
        "\n",
        "    if is_last_layer:\n",
        "      dA = 2 * (self.outputs - truth) / self.outputs.shape[0]\n",
        "    else:\n",
        "      dA = dZ_prev\n",
        "\n",
        "    dZ = dA * self.activation_function_derivative(self.activation, is_last_layer)\n",
        "    self.dW = np.dot(self.inputs.T, dZ)\n",
        "    self.db = np.sum(dZ, axis=0, keepdims=True)\n",
        "    self.update_weights(lr)\n",
        "\n",
        "    dZ = np.dot(dZ, self.weights.T)\n",
        "    return dZ\n",
        "\n",
        "  def update_weights(self, learning_rate):\n",
        "    self.weights -= learning_rate * self.dW\n",
        "    self.biases -= learning_rate * self.db\n",
        "\n",
        "  def activation_function_derivative(self, activation_function, is_last_layer):\n",
        "    if activation_function == \"sigmoid\":\n",
        "        return self.outputs * (1 - self.outputs)\n",
        "    elif activation_function == \"relu\":\n",
        "        return np.where(self.unactivated_outputs > 0, 1, 0)\n",
        "    elif activation_function == \"tanh\":\n",
        "        return 1 - self.outputs ** 2\n",
        "    else:\n",
        "        return self.outputs\n",
        "\n",
        "  def activation_function(self, result, activation_function, is_first_layer, is_last_layer):\n",
        "    if activation_function == \"sigmoid\":\n",
        "        return self.sigmoid(result)\n",
        "    elif activation_function == \"relu\":\n",
        "        return self.relu(result)\n",
        "    elif activation_function == \"tanh\":\n",
        "        return self.tanh(result)\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "  def sigmoid(self, result):\n",
        "    return 1 / (1 + np.exp(-result))\n",
        "\n",
        "  def relu(self, result):\n",
        "    return np.maximum(0, result)\n",
        "\n",
        "  def tanh(self, result):\n",
        "    return np.tanh(result)\n"
      ],
      "metadata": {
        "id": "fbOgAdEq2-oU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Back Propagation Formula**\n",
        "\n",
        "dA = 2 * (A - Y) / m               \n",
        "\n",
        "dA (apart from output layer) = (dZ(prev) @ W.T(prev))            \n",
        "\n",
        "dZ = dA * A * (1 - A)              # ∂L/∂Z (sigmoid derivative)\n",
        "\n",
        "dW = X.T @ dZ                      # ∂L/∂W\n",
        "\n",
        "db = dZ.sum(axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "X: input matrix, shape (m × n) — m examples, n features\n",
        "\n",
        "W: weight matrix, shape (n × k) — maps n inputs to k outputs (e.g., neurons)\n",
        "\n",
        "b: bias vector, shape (1 × k) (broadcasted)\n",
        "\n",
        "Z: linear output, shape (m × k)\n",
        "\n",
        "A: activated output, shape (m × k)\n",
        "\n",
        "Y: ground truth targets, shape (m × k)"
      ],
      "metadata": {
        "id": "SvQjkba5jXSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.random.rand(3, 4)  # batch size = 3, input features = 4\n",
        "neurons_per_layers = [\n",
        "    (5, \"relu\"),   # hidden layer: 5 neurons with ReLU\n",
        "    (2, \"sigmoid\") # output layer: 2 neurons with sigmoid\n",
        "]\n",
        "\n",
        "targets = np.random.rand(3, 2) # batch size = 3, output features = 2\n",
        "\n",
        "nn = NeuralNetwork(neurons_per_layers, inputs, 10000)\n",
        "\n",
        "print(\"Before Training:\")\n",
        "initial_output = nn.forward_pass()\n",
        "print(\"Initial Output:\\n\", initial_output)\n",
        "print(\"Initial Loss:\", nn.calculate_loss(targets))\n",
        "\n",
        "# Train the network\n",
        "nn.train(targets)\n",
        "\n",
        "print(\"\\nAfter Training:\")\n",
        "print(\"Final Loss:\", nn.loss)\n",
        "final_output = nn.forward_pass()\n",
        "print(\"Final Output:\\n\", final_output)\n",
        "print(\"Target Output:\\n\", targets)\n",
        "print(\"Difference:\\n\", np.abs(final_output - targets))\n",
        "\n",
        "print(\"\\nNetwork Summary:\")\n",
        "nn.summary()\n",
        "\n",
        "print(\"\\nFinal Weights:\")\n",
        "for i, layer in enumerate(nn.layers):\n",
        "    print(f\"Layer {i+1} Weights Shape:\", layer.weights.shape)\n",
        "    print(f\"Layer {i+1} Weights:\\n\", layer.weights)\n",
        "    print(f\"Layer {i+1} Biases:\\n\", layer.biases)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmG8-OdtKFiv",
        "outputId": "440c5b8e-1b4e-41bc-e3d7-549e70caee74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Training:\n",
            "Initial Output:\n",
            " [[0.769309   0.90208931]\n",
            " [0.73379308 0.84585946]\n",
            " [0.80376747 0.90651676]]\n",
            "Initial Loss: 0.15365358274675003\n",
            "Epoch 0, Loss: 0.15365358274675003\n",
            "Epoch 100, Loss: 0.12008242876999368\n",
            "Epoch 200, Loss: 0.09723849173015238\n",
            "Epoch 300, Loss: 0.0846215815520432\n",
            "Epoch 400, Loss: 0.07771399185642236\n",
            "Epoch 500, Loss: 0.07317147012552998\n",
            "Epoch 600, Loss: 0.06945266708193804\n",
            "Epoch 700, Loss: 0.06596708559891541\n",
            "Epoch 800, Loss: 0.06250849359548154\n",
            "Epoch 900, Loss: 0.05901581443791056\n",
            "Epoch 1000, Loss: 0.05548406784143062\n",
            "Epoch 1100, Loss: 0.05193183262903094\n",
            "Epoch 1200, Loss: 0.04838861525720519\n",
            "Epoch 1300, Loss: 0.045029149850032306\n",
            "Epoch 1400, Loss: 0.04200836805420599\n",
            "Epoch 1500, Loss: 0.039150212236878784\n",
            "Epoch 1600, Loss: 0.03642842689688325\n",
            "Epoch 1700, Loss: 0.03384637385812345\n",
            "Epoch 1800, Loss: 0.03140469866268026\n",
            "Epoch 1900, Loss: 0.02909399490649799\n",
            "Epoch 2000, Loss: 0.026931034826421774\n",
            "Epoch 2100, Loss: 0.02491007361411149\n",
            "Epoch 2200, Loss: 0.023036537388873183\n",
            "Epoch 2300, Loss: 0.021296613842417555\n",
            "Epoch 2400, Loss: 0.019701711131505897\n",
            "Epoch 2500, Loss: 0.01823394622792226\n",
            "Epoch 2600, Loss: 0.016899103362320207\n",
            "Epoch 2700, Loss: 0.01568084851434902\n",
            "Epoch 2800, Loss: 0.014568870386716984\n",
            "Epoch 2900, Loss: 0.013557648282160504\n",
            "Epoch 3000, Loss: 0.012637742240018\n",
            "Epoch 3100, Loss: 0.011800838319143367\n",
            "Epoch 3200, Loss: 0.011039275555933883\n",
            "Epoch 3300, Loss: 0.010344660237173264\n",
            "Epoch 3400, Loss: 0.009715895610275296\n",
            "Epoch 3500, Loss: 0.009142067735888427\n",
            "Epoch 3600, Loss: 0.008617192820933962\n",
            "Epoch 3700, Loss: 0.008139283759662668\n",
            "Epoch 3800, Loss: 0.007701291335255707\n",
            "Epoch 3900, Loss: 0.007301646877163861\n",
            "Epoch 4000, Loss: 0.006933894626215285\n",
            "Epoch 4100, Loss: 0.006597218298348486\n",
            "Epoch 4200, Loss: 0.00628663940823412\n",
            "Epoch 4300, Loss: 0.006000919087822063\n",
            "Epoch 4400, Loss: 0.0057366300733424315\n",
            "Epoch 4500, Loss: 0.005492911712787805\n",
            "Epoch 4600, Loss: 0.005265953678343985\n",
            "Epoch 4700, Loss: 0.005055405097448063\n",
            "Epoch 4800, Loss: 0.004860155670926896\n",
            "Epoch 4900, Loss: 0.004677626226974683\n",
            "Epoch 5000, Loss: 0.0045073847595856606\n",
            "Epoch 5100, Loss: 0.004348392983469493\n",
            "Epoch 5200, Loss: 0.004198988857780928\n",
            "Epoch 5300, Loss: 0.004059014922262916\n",
            "Epoch 5400, Loss: 0.003926956383980665\n",
            "Epoch 5500, Loss: 0.0038027262167964772\n",
            "Epoch 5600, Loss: 0.003685885485922145\n",
            "Epoch 5700, Loss: 0.003575100928701279\n",
            "Epoch 5800, Loss: 0.003470354668218415\n",
            "Epoch 5900, Loss: 0.003370837054623319\n",
            "Epoch 6000, Loss: 0.0032766134961843174\n",
            "Epoch 6100, Loss: 0.003187012716452419\n",
            "Epoch 6200, Loss: 0.0031016897101814093\n",
            "Epoch 6300, Loss: 0.003020223314730735\n",
            "Epoch 6400, Loss: 0.002942684213738327\n",
            "Epoch 6500, Loss: 0.002868410535800959\n",
            "Epoch 6600, Loss: 0.002797291809380346\n",
            "Epoch 6700, Loss: 0.0027293699771082295\n",
            "Epoch 6800, Loss: 0.002664266876807027\n",
            "Epoch 6900, Loss: 0.002601630170467006\n",
            "Epoch 7000, Loss: 0.002541603078923285\n",
            "Epoch 7100, Loss: 0.0024837825580403573\n",
            "Epoch 7200, Loss: 0.0024281154441495438\n",
            "Epoch 7300, Loss: 0.0023746406268284996\n",
            "Epoch 7400, Loss: 0.0023230355185959698\n",
            "Epoch 7500, Loss: 0.002273206107261771\n",
            "Epoch 7600, Loss: 0.002225127764506594\n",
            "Epoch 7700, Loss: 0.0021786344757130584\n",
            "Epoch 7800, Loss: 0.0021337384772135647\n",
            "Epoch 7900, Loss: 0.0020902588235948935\n",
            "Epoch 8000, Loss: 0.002048135736898959\n",
            "Epoch 8100, Loss: 0.002007382599256416\n",
            "Epoch 8200, Loss: 0.0019679529385707687\n",
            "Epoch 8300, Loss: 0.0019296638173992764\n",
            "Epoch 8400, Loss: 0.001892477400805782\n",
            "Epoch 8500, Loss: 0.0018564754364539414\n",
            "Epoch 8600, Loss: 0.0018214151129350362\n",
            "Epoch 8700, Loss: 0.001787464240235564\n",
            "Epoch 8800, Loss: 0.0017543723302209718\n",
            "Epoch 8900, Loss: 0.0017222831338151202\n",
            "Epoch 9000, Loss: 0.0016910268126789573\n",
            "Epoch 9100, Loss: 0.0016605898998329558\n",
            "Epoch 9200, Loss: 0.0016310177553338259\n",
            "Epoch 9300, Loss: 0.0016022283279893967\n",
            "Epoch 9400, Loss: 0.0015741623407026248\n",
            "Epoch 9500, Loss: 0.0015468041458913892\n",
            "Epoch 9600, Loss: 0.0015201719665798713\n",
            "Epoch 9700, Loss: 0.0014942251840877937\n",
            "Epoch 9800, Loss: 0.001468905883566611\n",
            "Epoch 9900, Loss: 0.0014442349378920186\n",
            "\n",
            "After Training:\n",
            "Final Loss: 0.0014203792557893615\n",
            "Final Output:\n",
            " [[0.43397682 0.79716253]\n",
            " [0.89174756 0.09082731]\n",
            " [0.44508404 0.84984656]]\n",
            "Target Output:\n",
            " [[0.445284   0.75278371]\n",
            " [0.90036345 0.05384025]\n",
            " [0.43034484 0.91886815]]\n",
            "Difference:\n",
            " [[0.01130718 0.04437882]\n",
            " [0.00861589 0.03698706]\n",
            " [0.0147392  0.06902159]]\n",
            "\n",
            "Network Summary:\n",
            "Layer 1: 5 neurons, activation: relu\n",
            "Layer 2: 2 neurons, activation: sigmoid\n",
            "\n",
            "Final Weights:\n",
            "Layer 1 Weights Shape: (4, 5)\n",
            "Layer 1 Weights:\n",
            " [[ 0.46867521  1.3858198   0.09684532  0.07876452 -0.18854557]\n",
            " [ 0.56531873 -0.58724565  0.4217506   1.23092789  1.6528153 ]\n",
            " [ 0.18056565 -0.02578351  0.66176178  0.49329331  1.22906532]\n",
            " [ 0.89379358  0.72250497 -0.00597372 -0.09072315 -0.09983055]]\n",
            "Layer 1 Biases:\n",
            " [[ 0.01964079  0.23846908 -0.17937215 -0.22307479 -0.20918921]]\n",
            "\n",
            "Layer 2 Weights Shape: (5, 2)\n",
            "Layer 2 Weights:\n",
            " [[ 0.12396833 -0.37286806]\n",
            " [ 1.40092975 -0.79996722]\n",
            " [ 0.5276305   0.69002841]\n",
            " [-0.38510207  1.2368973 ]\n",
            " [-0.79283653  1.68839293]]\n",
            "Layer 2 Biases:\n",
            " [[ 0.1159329  -0.88792638]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1JSZNvdKwE8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}